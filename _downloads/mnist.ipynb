{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\nClassification of handwritten digits\n====================================\n\nBased on pytorch example for MNIST\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\nimport torch.optim\nfrom torchvision import datasets, transforms\nimport torch.nn.functional as F\nfrom kymatio import Scattering2D\nimport kymatio.datasets as scattering_datasets\nimport kymatio\nimport torch\nimport argparse\nimport math\n\nclass View(nn.Module):\n    def __init__(self, *args):\n        super(View, self).__init__()\n        self.shape = args\n\n    def forward(self, x):\n        return x.view(-1,*self.shape)\n\ndef train(model, device, train_loader, optimizer, epoch, scattering):\n    model.train()\n    for batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n        optimizer.zero_grad()\n        output = model(scattering(data))\n        loss = F.cross_entropy(output, target)\n        loss.backward()\n        optimizer.step()\n        if batch_idx % 50 == 0:\n            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx / len(train_loader), loss.item()))\n\ndef test(model, device, test_loader, scattering):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n            output = model(scattering(data))\n            test_loss += F.cross_entropy(output, target, reduction='sum').item() # sum up batch loss\n            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n            correct += pred.eq(target.view_as(pred)).sum().item()\n\n    test_loss /= len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset),\n        100. * correct / len(test_loader.dataset)))\n\ndef main():\n    \"\"\"Train a simple Hybrid Scattering + CNN model on MNIST.\n\n        Three models are demoed:\n        'linear' - scattering + linear model\n        'mlp' - scattering + MLP\n        'cnn' - scattering + CNN\n\n        scattering 1st order can also be set by the mode\n        Scattering features are normalized by batch normalization.\n\n        scatter + linear achieves 99.15% in 15 epochs\n        scatter + cnn achieves 99.3% in 15 epochs\n\n    \"\"\"\n    parser = argparse.ArgumentParser(description='MNIST scattering  + hybrid examples')\n    parser.add_argument('--mode', type=int, default=2,help='scattering 1st or 2nd order')\n    parser.add_argument('--classifier', type=str, default='linear',help='classifier model')\n    args = parser.parse_args()\n    assert(args.classifier in ['linear','mlp','cnn'])\n\n    use_cuda = torch.cuda.is_available()\n    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\n    if args.mode == 1:\n        scattering = Scattering2D(J=2, shape=(28, 28), max_order=1)\n        K = 17\n    else:\n        scattering = Scattering2D(J=2, shape=(28, 28))\n        K = 81\n    if use_cuda:\n        scattering = scattering.cuda()\n\n\n\n\n    if args.classifier == 'cnn':\n        model = nn.Sequential(\n            View(K, 7, 7),\n            nn.BatchNorm2d(K),\n            nn.Conv2d(K, 64, 3,padding=1), nn.ReLU(),\n            View(64*7*7),\n            nn.Linear(64 * 7 * 7, 512), nn.ReLU(),\n            nn.Linear(512, 10)\n        ).to(device)\n\n    elif args.classifier == 'mlp':\n        model = nn.Sequential(\n            View(K, 7, 7),\n            nn.BatchNorm2d(K),\n            View(K*7*7),\n            nn.Linear(K*7*7, 512), nn.ReLU(),\n            nn.Linear(512, 512), nn.ReLU(),\n            nn.Linear(512, 10)\n        )\n\n    elif args.classifier == 'linear':\n        model = nn.Sequential(\n            View(K, 7, 7),\n            nn.BatchNorm2d(K),\n            View(K * 7 * 7),\n            nn.Linear(K * 7 * 7, 10)\n        )\n    else:\n        raise ValueError('Classifier should be cnn/mlp/linear')\n\n    model.to(device)\n\n    #initialize\n    for m in model.modules():\n        if isinstance(m, nn.Conv2d):\n            n = m.kernel_size[0] * m.kernel_size[1] * m.in_channels\n            m.weight.data.normal_(0, 2./math.sqrt(n))\n            m.bias.data.zero_()\n        if isinstance(m, nn.Linear):\n            m.weight.data.normal_(0, 2./math.sqrt(m.in_features))\n            m.bias.data.zero_()\n\n    # DataLoaders\n    if use_cuda:\n        num_workers = 4\n        pin_memory = True\n    else:\n        num_workers = None\n        pin_memory = False\n\n    train_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(scattering_datasets.get_dataset_dir('MNIST'), train=True, download=True,\n                       transform=transforms.Compose([\n                           transforms.ToTensor(),\n                           transforms.Normalize((0.1307,), (0.3081,))\n                       ])),\n        batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n    test_loader = torch.utils.data.DataLoader(\n        datasets.MNIST(scattering_datasets.get_dataset_dir('MNIST'), train=False, transform=transforms.Compose([\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])),\n        batch_size=128, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n\n    # Optimizer\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9,\n                                weight_decay=0.0005)\n\n    for epoch in range(1, 16):\n        train( model, device, train_loader, optimizer, epoch, scattering)\n        test(model, device, test_loader, scattering)\n\n\nif __name__ == '__main__':\n    main()"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.5.2",
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "nbformat_minor": 0
}