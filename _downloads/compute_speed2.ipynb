{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n3D scattering transform benchmark\n=================================\nWe compute scattering transforms for volume maps of size `128`-by-`128`-by-\n`128`, with averaging scale `2**2 = 4` and maximum spherical harmonic\norder `L = 2`. The volumes are stacked into batches of size `batch_size = 8`\nand the transforms are computed `10` times to get an average running time.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Preliminaries\n-------------\nSince kymatio handles PyTorch arrays, we first import `torch`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To measure the running time of the implementation, we use the `time` package.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The performance of the implementation depends on which \"backend\" is used. We\ntherefore want to report the name of the backend when presenting the results.\nCertain backends are also GPU-only, we we want to detect that before running\nthe benchmark.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import kymatio.scattering3d.backend as backend"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, we import the `Scattering3D` class that computes the scattering\ntransform.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from kymatio import Scattering3D"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Benchmark setup\n--------------------\nFirst, we set up some basic parameters: the volume width `M`, height `N`,\nand depth 'O', the maximum number of the spherical harmonics `L`, and the\nmaximum scale `2**J`. Here, we consider cubic volumes of size `128`, with\na maximum scale of `2**2 = 4` and maximum spherical harmonic order of `2`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "M, N, O = 128, 128, 128\nJ = 2\nL = 2\n\nintegral_powers = [1., 2.]\nsigma_0 = 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To squeeze the maximum performance out of the implementation, we apply it to\na batch of `8` volumes. Larger batch sizes do not yield increased efficiency,\nbut smaller values increases the influence of overhead on the running time.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We repeat the benchmark `10` times and compute the average running time to\nget a reasonable estimate.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "times = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Determine which devices (CPU or GPU) that are supported by the current\nbackend.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "if backend.NAME == 'torch':\n    devices = ['cpu', 'gpu']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the scattering object and the test data\n----------------------------------------------\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Create the `Scattering3D` object using the given parameters and generate\nsome compatible test data with the specified batch size.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scattering = Scattering3D(M, N, O, J, L, sigma_0)\n\nx = torch.randn(batch_size, M, N, O, dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Run the benchmark\n-----------------\nFor each device, we need to convert the Tensor `x` to the appropriate type,\ninvoke `times` calls to `scattering.forward` and print the running times.\nBefore the timer starts, we add an extra `scattering.forward` call to ensure\nany first-time overhead, such as memory allocation and CUDA kernel\ncompilation, is not counted. If the benchmark is running on the GPU, we also\nneed to call `torch.cuda.synchronize()` before and after the benchmark to\nmake sure that all CUDA kernels have finished executing.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for device in devices:\n    fmt_str = '==> Testing Float32 with {} backend, on {}, forward'\n    print(fmt_str.format(backend.NAME, device.upper()))\n\n    if device == 'gpu':\n        x = x.cuda()\n    else:\n        x = x.cpu()\n\n    scattering.forward(x, method='integral', integral_powers=integral_powers)\n\n    if device == 'gpu':\n        torch.cuda.synchronize()\n\n    t_start = time.time()\n    for _ in range(times):\n        scattering.forward(x, method='integral', integral_powers=integral_powers)\n\n    if device == 'gpu':\n        torch.cuda.synchronize()\n\n    t_elapsed = time.time() - t_start\n\n    fmt_str = 'Elapsed time: {:2f} [s / {:d} evals], avg: {:.2f} (s/batch)'\n    print(fmt_str.format(t_elapsed, times, t_elapsed/times))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting output should be something like\n\n.. code-block:: text\n\n  ==> Testing Float32 with torch backend, on CPU, forward\n  Elapsed time: 109.739110 [s / 10 evals], avg: 10.97 (s/batch)\n  ==> Testing Float32 with torch backend, on GPU, forward\n  Elapsed time: 60.476041 [s / 10 evals], avg: 6.05 (s/batch)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}