{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n2D scattering transform benchmark\n=================================\nWe compute scattering transforms for images of size `256`-by-`256` with\naveraging scale `2**3 = 8` and `L = 8` angular directions. The images are\nstacked into batches of size `batch_size = 128` and the transforms are\ncomputed `10` times to get an average running time.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preliminaries\n-------------\nSince kymatio handles PyTorch arrays, we first import `torch`.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To measure the running time of the implementation, we use the `time` package.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The performance of the implementation depends on which \"backend\" is used. We\ntherefore want to report the name of the backend when presenting the results.\nCertain backends are also GPU-only, we we want to detect that before running\nthe benchmark.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import kymatio.scattering2d.backend as backend"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we import the `Scattering2D` class that computes the scattering\ntransform.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from kymatio import Scattering2D"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Benchmark setup\n--------------------\nFirst, we set up some basic parameters: the image width `M` and height `N`,\nthe averaging scale, `2**J`, and the number of angular directions `L`.\nHere, we consider square images of size `256` with an averaging scale\n`2**3 = 8` and `L = 8` angular directions. These are all typical parameter\nfor scattering transforms of natural images.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "M = 256\nN = 256\nJ = 3\nL = 8"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To squeeze the maximum performance out of the implementation, we apply it to\na batch of `128` images.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We repeat the benchmark `10` times and compute the average running time to\nget a reasonable estimate.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "times = 10"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Determine which devices (CPU or GPU) that are supported by the current\nbackend.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "if backend.NAME == 'torch':\n    devices = ['cpu', 'gpu']\nelif backend.NAME == 'skcuda':\n    devices = ['gpu']"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the `Scattering2D` object using the given parameters and generate\nsome compatible test data with the specified batch size. The number of\nchannels in the test data here is set to `3`, corresponding to the three\ncolors channels in an RGB image.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "scattering = Scattering2D(J, shape=(M, N), L=L)\n\nx = torch.randn(batch_size, 3, M, N, dtype=torch.float32)"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the benchmark\n-----------------\nFor each device, we need to convert the `scattering` object and the Tensor\n`x` to the appropriate type, invoke `times` calls to `scattering.forward`\nand print the running times. Before the timer starts, we add an extra\n`scattering.forward` call to ensure any first-time overhead, such as memory\nallocation and CUDA kernel compilation, is not counted. If the benchmark is\nrunning on the GPU, we also need to call `torch.cuda.synchronize()` before\nand after the benchmark to make sure that all CUDA kernels have finished\nexecuting.\n\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "for device in devices:\n    fmt_str = '==> Testing Float32 with {} backend, on {}, forward'\n    print(fmt_str.format(backend.NAME, device.upper()))\n\n    if device == 'gpu':\n        scattering.cuda()\n        x = x.cuda()\n    else:\n        scattering.cpu()\n        x = x.cpu()\n\n    scattering.forward(x)\n\n    if device == 'gpu':\n        torch.cuda.synchronize()\n\n    t_start = time.time()\n    for _ in range(times):\n        scattering.forward(x)\n\n    if device == 'gpu':\n        torch.cuda.synchronize()\n\n    t_elapsed = time.time() - t_start\n\n    fmt_str = 'Elapsed time: {:2f} [s / {:d} evals], avg: {:.2f} (s/batch)'\n    print(fmt_str.format(t_elapsed, times, t_elapsed/times))"
      ],
      "metadata": {
        "collapsed": false
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting output should be something like\n\n.. code-block:: text\n\n  ==> Testing Float32 with torch backend, on CPU, forward\n  Elapsed time: 624.910853 [s / 10 evals], avg: 62.49 (s/batch)\n  ==> Testing Float32 with torch backend, on GPU, forward\n  Elapsed time: 130.580992 [s / 10 evals], avg: 13.06 (s/batch)\n\n"
      ],
      "metadata": {}
    }
  ],
  "nbformat": 4,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "mimetype": "text/x-python",
      "name": "python",
      "version": "3.5.2",
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    }
  },
  "nbformat_minor": 0
}